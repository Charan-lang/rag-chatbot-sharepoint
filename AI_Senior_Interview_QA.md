# AI Senior Engineer Interview Questions & Answers
## 3+ Years Experience - Comprehensive Study Guide
### ðŸ“Œ With Real-Time Project Examples & Implementation Insights

---

# Table of Contents
1. [Machine Learning Fundamentals](#1-machine-learning-fundamentals)
2. [Deep Learning & Neural Networks](#2-deep-learning--neural-networks)
3. [Natural Language Processing (NLP)](#3-natural-language-processing-nlp)
4. [Large Language Models (LLMs)](#4-large-language-models-llms)
5. [RAG (Retrieval Augmented Generation)](#5-rag-retrieval-augmented-generation)
6. [Vector Databases & Embeddings](#6-vector-databases--embeddings)
7. [Azure AI Services](#7-azure-ai-services)
8. [MLOps & Production AI](#8-mlops--production-ai)
9. [Python & Frameworks](#9-python--frameworks)
10. [System Design for AI](#10-system-design-for-ai)
11. [Behavioral & Scenario Questions](#11-behavioral--scenario-questions)

> ðŸ’¡ **Note:** Each answer includes **"ðŸ”§ Real Project Application"** sections showing how I applied these concepts in production projects like Enterprise RAG Chatbots, SharePoint Document Intelligence Systems, and Permission-Aware Search Solutions.

---

# 1. Machine Learning Fundamentals

## Q1: What is the difference between Supervised, Unsupervised, and Reinforcement Learning?

**Answer:**

| Type | Description | Examples |
|------|-------------|----------|
| **Supervised Learning** | Model learns from labeled data (input-output pairs) | Classification, Regression |
| **Unsupervised Learning** | Model finds patterns in unlabeled data | Clustering, Dimensionality Reduction |
| **Reinforcement Learning** | Agent learns through rewards/penalties from environment | Game AI, Robotics |

**Example:** 
- Supervised: Predicting house prices (regression) with features like size, location
- Unsupervised: Customer segmentation without predefined groups
- Reinforcement: Training a robot to walk through trial and error

**ðŸ”§ Real Project Application:**
> In my **Enterprise RAG Chatbot** project, I used **supervised learning** principles for document classification - categorizing SharePoint documents by department before indexing. The labels (Finance, HR, Engineering) were used to train a classifier that auto-tags new uploads, enabling department-specific search filters. This improved retrieval precision by 40% as users could narrow searches to their domain.

---

## Q2: Explain Bias-Variance Tradeoff

**Answer:**

**Bias:** Error from oversimplified assumptions (underfitting)
- High bias = model misses relevant relations
- Example: Linear model for non-linear data

**Variance:** Error from sensitivity to training data fluctuations (overfitting)
- High variance = model captures noise as patterns
- Example: Deep tree memorizing training data

**Tradeoff:**
```
Total Error = BiasÂ² + Variance + Irreducible Error
```

**Solutions:**
- **High Bias:** Increase model complexity, add features, reduce regularization
- **High Variance:** More training data, regularization (L1/L2), dropout, cross-validation

**ðŸ”§ Real Project Application:**
> When building a **document relevance scoring model** for our RAG system, the initial linear model (high bias) couldn't capture nuanced relationships between queries and documents. We upgraded to a neural re-ranker but it started memorizing training examples (high variance). Solution: Applied **dropout (0.3)** and **early stopping** based on validation loss, achieving optimal balance with F1 score improving from 0.72 to 0.89.

---

## Q3: What are Precision, Recall, F1-Score? When to use each?

**Answer:**

```
Precision = TP / (TP + FP)  â†’ "Of all positive predictions, how many correct?"
Recall    = TP / (TP + FN)  â†’ "Of all actual positives, how many found?"
F1-Score  = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**When to use:**
| Metric | Use When |
|--------|----------|
| **Precision** | False positives are costly (spam detection, fraud alerts) |
| **Recall** | False negatives are costly (cancer detection, security threats) |
| **F1-Score** | Need balance between precision and recall |
| **AUC-ROC** | Comparing models across thresholds |

**ðŸ”§ Real Project Application:**
> In our **Permission-Aware Document Search**, we prioritized **Recall** for the retrieval component - we'd rather show a few irrelevant documents than miss a critical one the user needs. However, for the **security filtering** (ensuring users only see authorized documents), we optimized for **Precision** - a false positive (showing unauthorized document) is a security breach, while false negative (hiding an authorized doc) just means re-triggering sync.

---

## Q4: Explain Cross-Validation and its types

**Answer:**

Cross-validation evaluates model performance on unseen data by splitting data into folds.

**Types:**
1. **K-Fold CV:** Split into K parts, train on K-1, test on 1, rotate
2. **Stratified K-Fold:** Maintains class distribution in each fold
3. **Leave-One-Out (LOOCV):** K = number of samples (expensive)
4. **Time Series CV:** Respects temporal order (no future data leakage)

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=cv, scoring='f1')
print(f"Mean F1: {scores.mean():.3f} Â± {scores.std():.3f}")
```

**ðŸ”§ Real Project Application:**
> For evaluating our **query intent classifier** (determines if user wants search vs. summary vs. comparison), I used **Stratified 5-Fold CV** because intent classes were imbalanced (70% search, 20% summary, 10% comparison). This ensured each fold had representative samples. The cross-validation revealed high variance across folds (0.85 Â± 0.12), prompting us to collect more "comparison" examples before production deployment.

---

## Q5: What is Regularization? Explain L1 vs L2

**Answer:**

Regularization prevents overfitting by adding penalty to loss function.

| Type | Formula | Effect | Use Case |
|------|---------|--------|----------|
| **L1 (Lasso)** | Î»âˆ‘\|w\| | Sparse weights (feature selection) | Many irrelevant features |
| **L2 (Ridge)** | Î»âˆ‘wÂ² | Small weights (no zeros) | All features relevant |
| **Elastic Net** | Î±Ã—L1 + (1-Î±)Ã—L2 | Combination | Best of both |

**Example:**
```python
from sklearn.linear_model import Lasso, Ridge, ElasticNet

lasso = Lasso(alpha=0.1)  # L1 - some coefficients become 0
ridge = Ridge(alpha=0.1)  # L2 - coefficients shrink but non-zero
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Mix
```

**ðŸ”§ Real Project Application:**
> When building a **document importance scorer** with 50+ features (word count, readability, freshness, access frequency), many features were noise. Used **L1 (Lasso)** regularization which zeroed out 35 features, revealing that only `access_frequency`, `last_modified_days`, `unique_term_count`, and 12 others actually mattered. This simplified model was faster and more interpretable for stakeholders.

---

## Q6: How do you handle imbalanced datasets?

**Answer:**

**Techniques:**

1. **Resampling:**
   - Oversampling minority (SMOTE, ADASYN)
   - Undersampling majority (Random, Tomek links)

2. **Algorithm-level:**
   - Class weights (`class_weight='balanced'`)
   - Cost-sensitive learning

3. **Ensemble methods:**
   - BalancedRandomForest
   - EasyEnsemble

4. **Evaluation:**
   - Use F1, AUC-ROC instead of accuracy
   - Precision-Recall curves

```python
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Or use class weights
model = RandomForestClassifier(class_weight='balanced')
```

**ðŸ”§ Real Project Application:**
> In our **document anomaly detection** system (flagging potentially sensitive documents), we had 99% normal documents and only 1% sensitive. Using accuracy was misleading (99% by predicting all normal!). Applied **class_weight='balanced'** and switched to **Precision-Recall AUC** as the metric. Also used **SMOTE** to generate synthetic sensitive document features, improving detection recall from 45% to 82% without flooding users with false alerts.

---

# 2. Deep Learning & Neural Networks

## Q7: Explain Backpropagation

**Answer:**

Backpropagation is the algorithm to compute gradients for updating neural network weights.

**Steps:**
1. **Forward Pass:** Compute predictions layer by layer
2. **Loss Calculation:** Compare predictions with targets
3. **Backward Pass:** Compute gradients using chain rule
4. **Weight Update:** Adjust weights using optimizer (SGD, Adam)

```
âˆ‚Loss/âˆ‚w = âˆ‚Loss/âˆ‚output Ã— âˆ‚output/âˆ‚activation Ã— âˆ‚activation/âˆ‚w
```

**Key concepts:**
- Chain rule for gradient computation
- Computational graph tracks operations
- Gradients flow backwards through layers

**ðŸ”§ Real Project Application:**
> Understanding backpropagation was crucial when **debugging a custom re-ranker model** that wasn't learning. By inspecting gradients at each layer, I discovered the loss function gradient was near-zero for most samples (the margin-based loss was too easy to satisfy). Switching to a **contrastive loss** with hard negative mining fixed the gradient flow, and the model started learning meaningful document-query relevance patterns.

---

## Q8: What is Vanishing/Exploding Gradient Problem? How to solve?

**Answer:**

**Vanishing Gradients:** Gradients become very small in deep networks (sigmoid/tanh)
**Exploding Gradients:** Gradients become very large (unstable training)

**Solutions:**

| Problem | Solutions |
|---------|-----------|
| Vanishing | ReLU activation, Batch Normalization, Residual connections, LSTM/GRU |
| Exploding | Gradient clipping, Weight initialization (Xavier/He), Batch Normalization |

```python
# Gradient Clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Proper initialization
nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')

# Batch Normalization
self.bn = nn.BatchNorm1d(hidden_size)
```

**ðŸ”§ Real Project Application:**
> While fine-tuning a **BERT-based document classifier** on our domain data, training became unstable with NaN losses after a few epochs (exploding gradients). Implemented **gradient clipping (max_norm=1.0)** and added **warmup learning rate schedule** (500 steps). Also switched from SGD to **AdamW** with weight decay. Training stabilized and achieved 94% classification accuracy on internal document categories.

---

## Q9: Compare CNN vs RNN vs Transformer architectures

**Answer:**

| Architecture | Best For | Key Features | Limitations |
|--------------|----------|--------------|-------------|
| **CNN** | Images, spatial data | Local patterns, translation invariance, parameter sharing | Fixed receptive field |
| **RNN** | Sequential data | Memory of past inputs | Vanishing gradients, slow (sequential) |
| **Transformer** | NLP, long sequences | Self-attention, parallel processing | O(nÂ²) attention, memory intensive |

**When to use:**
- **CNN:** Image classification, object detection, 1D signals
- **RNN/LSTM:** Time series, short sequences, streaming data
- **Transformer:** NLP tasks, long-range dependencies, when parallelization needed

**ðŸ”§ Real Project Application:**
> For our **document ingestion pipeline**, I chose **Transformers** (BERT-based) over RNNs for text understanding because:
> 1. Documents have long-range dependencies (intro references conclusion)
> 2. Parallel processing crucial for batch ingestion of 10K+ documents
> 3. Pre-trained models available for transfer learning
> 
> However, for **real-time user activity tracking** (predicting next document a user might access), we used **LSTM** because it handles streaming sequential data naturally and has lower latency for single predictions.

---

## Q10: Explain Attention Mechanism and Self-Attention

**Answer:**

**Attention:** Mechanism to focus on relevant parts of input when producing output.

**Self-Attention (Transformer):**
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
```

Where:
- **Q (Query):** What we're looking for
- **K (Key):** What we match against
- **V (Value):** What we retrieve
- **âˆšd_k:** Scaling factor for stability

**Multi-Head Attention:** Multiple attention heads capture different relationships
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
    def forward(self, Q, K, V):
        # Split into heads, compute attention, concatenate
        ...
```

**ðŸ”§ Real Project Application:**
> In our **semantic search system**, understanding attention helped debug why certain queries retrieved irrelevant documents. By visualizing attention weights, we discovered the model was over-attending to common words like "the", "document", "please". Solution: Implemented **attention masking for stop words** in our custom embedding model and added **term frequency weighting**, improving retrieval MRR from 0.65 to 0.78.

---

## Q11: What are different Optimizers? Compare SGD, Adam, AdamW

**Answer:**

| Optimizer | Description | Pros | Cons |
|-----------|-------------|------|------|
| **SGD** | Basic gradient descent | Simple, good generalization | Slow, sensitive to LR |
| **SGD+Momentum** | Accelerates in consistent direction | Faster convergence | Still needs LR tuning |
| **Adam** | Adaptive learning rates + momentum | Fast, works well out-of-box | May not generalize well |
| **AdamW** | Adam with decoupled weight decay | Better generalization | Slightly more computation |

**Best Practices:**
- **Computer Vision:** SGD with momentum often better
- **NLP/Transformers:** AdamW preferred
- **Quick experiments:** Adam for fast convergence

```python
# AdamW with learning rate scheduler
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
```

**ðŸ”§ Real Project Application:**
> For fine-tuning our **domain-specific embedding model**, I compared optimizers:
> - **SGD:** Too slow, 50 epochs to converge
> - **Adam:** Fast but overfit after 10 epochs
> - **AdamW:** Best results with `lr=2e-5, weight_decay=0.01`
> 
> Combined with **CosineAnnealingLR** scheduler, the model converged in 15 epochs with better generalization. The final model improved document retrieval accuracy by 23% over the base OpenAI embeddings for our domain-specific terminology.

---

# 3. Natural Language Processing (NLP)

## Q12: Explain Word Embeddings (Word2Vec, GloVe, FastText)

**Answer:**

Word embeddings represent words as dense vectors capturing semantic meaning.

| Method | Training | Strengths |
|--------|----------|-----------|
| **Word2Vec** | Skip-gram or CBOW | Fast, captures analogies |
| **GloVe** | Global co-occurrence matrix | Captures global statistics |
| **FastText** | Character n-grams | Handles OOV words, morphology |

**Key Properties:**
- Similar words have similar vectors
- Analogies: king - man + woman â‰ˆ queen
- Typically 100-300 dimensions

```python
from gensim.models import Word2Vec

model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)
similar = model.wv.most_similar('python')
```

**ðŸ”§ Real Project Application:**
> In our early RAG prototype, I experimented with **FastText** for handling domain-specific terminology (medical abbreviations like "ECG", "MRI", "CT"). FastText's character n-grams handled abbreviations and typos better than Word2Vec. However, for production, we switched to **OpenAI text-embedding-3-small** (1536 dimensions) because:
> 1. Better semantic understanding
> 2. Handles full sentences, not just words
> 3. No training required, reducing maintenance

---

## Q13: What is BERT? How does it work?

**Answer:**

**BERT (Bidirectional Encoder Representations from Transformers)**

**Architecture:**
- Transformer encoder (12 or 24 layers)
- Bidirectional context (sees both left and right)
- Pre-trained on large corpus, fine-tuned for tasks

**Pre-training Tasks:**
1. **Masked Language Modeling (MLM):** Predict masked tokens
2. **Next Sentence Prediction (NSP):** Predict if sentences are consecutive

**Usage:**
```python
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

inputs = tokenizer("Hello world", return_tensors="pt")
outputs = model(**inputs)
# outputs.last_hidden_state: [batch, seq_len, 768]
# outputs.pooler_output: [batch, 768] for classification
```

**Variants:** RoBERTa, ALBERT, DistilBERT, DeBERTa

**ðŸ”§ Real Project Application:**
> Used **DistilBERT** (66M params vs BERT's 110M) for our **real-time query understanding module**:
> ```python
> # Classify query intent: search, summarize, compare, question
> query_embedding = distilbert.encode(user_query)
> intent = intent_classifier.predict(query_embedding)
> ```
> DistilBERT gave us 95% of BERT's accuracy with 60% faster inference, crucial for keeping response latency under 200ms. For document indexing (offline), we used full **BERT-large** where speed wasn't critical.

---

## Q14: Explain Tokenization strategies (BPE, WordPiece, SentencePiece)

**Answer:**

| Method | Description | Used By |
|--------|-------------|---------|
| **BPE** | Iteratively merge frequent byte pairs | GPT, RoBERTa |
| **WordPiece** | Similar to BPE, likelihood-based | BERT |
| **SentencePiece** | Language-agnostic, raw text | T5, mBART |
| **Unigram** | Probabilistic, keeps most likely subwords | XLNet |

**Example (BPE):**
```
"unhappiness" â†’ ["un", "happiness"] â†’ ["un", "happ", "iness"]
```

**Key Considerations:**
- Vocabulary size tradeoff (smaller = more subwords per word)
- Unknown token handling
- Special tokens ([CLS], [SEP], [PAD])

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokens = tokenizer.tokenize("unhappiness")  # ['un', 'happ', 'iness']
```

**ðŸ”§ Real Project Application:**
> Tokenization understanding was critical when our **document chunks were getting truncated**. GPT-4 has 128K context, but our 1000-character chunks were becoming 1500+ tokens due to technical terms being split:
> - "SharePoint" â†’ ["Share", "Point"] (2 tokens)
> - "authentication" â†’ ["auth", "entic", "ation"] (3 tokens)
> 
> Solution: Adjusted chunk size based on **token count** not character count, and monitored token-to-character ratios per document type. Technical docs had 1.5x ratio vs 1.2x for general text.

---

# 4. Large Language Models (LLMs)

## Q15: Explain the GPT Architecture

**Answer:**

**GPT (Generative Pre-trained Transformer)**

**Architecture:**
- Decoder-only Transformer
- Unidirectional (causal) attention - can only see past tokens
- Autoregressive generation

**Key Components:**
```
Input â†’ Token Embedding + Position Embedding â†’ 
N Ã— (Masked Self-Attention â†’ Feed Forward) â†’ 
Output Logits â†’ Softmax â†’ Next Token Prediction
```

**Differences from BERT:**
| BERT | GPT |
|------|-----|
| Encoder-only | Decoder-only |
| Bidirectional | Unidirectional |
| MLM + NSP | Next token prediction |
| Classification, QA | Text generation |

**ðŸ”§ Real Project Application:**
> In our RAG system, we use **both paradigms**:
> - **BERT-style (Encoder):** For generating document embeddings - bidirectional context captures full document meaning
> - **GPT-style (Decoder):** For answer generation - autoregressive generation produces fluent responses
> 
> This hybrid approach (BERT for retrieval, GPT for generation) is the foundation of our production chatbot architecture, giving us best-of-both-worlds: semantic search + natural language answers.

---

## Q16: What is Prompt Engineering? Best practices?

**Answer:**

Prompt Engineering is the art of crafting inputs to get desired outputs from LLMs.

**Techniques:**

1. **Zero-shot:** Direct question without examples
2. **Few-shot:** Include examples in prompt
3. **Chain-of-Thought:** Ask model to show reasoning
4. **Role-playing:** Assign persona to model

**Best Practices:**
```python
# Zero-shot
prompt = "Classify the sentiment: 'This movie was great!' â†’ "

# Few-shot
prompt = """
Classify sentiment:
Text: "I love this!" â†’ Positive
Text: "Terrible experience" â†’ Negative
Text: "This movie was great!" â†’ """

# Chain-of-Thought
prompt = """
Solve step by step:
Q: If John has 5 apples and gives 2 to Mary, how many does he have?
A: Let's think step by step:
1. John starts with 5 apples
2. He gives 2 to Mary
3. 5 - 2 = 3
John has 3 apples.

Q: If a train travels 60 mph for 2.5 hours, how far does it go?
A: Let's think step by step:"""
```

**ðŸ”§ Real Project Application:**
> Our production system prompt for the RAG chatbot uses **multiple techniques**:
> ```python
> SYSTEM_PROMPT = """
> You are a helpful document assistant for {company_name}.
> 
> ROLE: Answer questions based ONLY on the provided context documents.
> 
> RULES:
> 1. If information is not in the context, say "I don't have information about that in the available documents."
> 2. Always cite your sources using [Document: filename, Page: X]
> 3. For complex questions, break down your reasoning step by step
> 4. If multiple documents have conflicting information, mention both perspectives
> 
> CONTEXT DOCUMENTS:
> {retrieved_chunks}
> 
> USER QUESTION: {user_query}
> """
> ```
> This prompt template reduced hallucinations by 75% compared to our initial naive prompt.

---

## Q17: Explain Fine-tuning vs RAG vs Agents

**Answer:**

| Approach | Description | Use Case | Pros | Cons |
|----------|-------------|----------|------|------|
| **Fine-tuning** | Train model on domain data | Specific style/domain | Customized behavior | Expensive, outdated knowledge |
| **RAG** | Retrieve context + generate | Dynamic knowledge | Up-to-date, verifiable | Retrieval quality dependent |
| **Agents** | LLM + tools + reasoning | Complex tasks | Flexible, multi-step | Unpredictable, slower |

**When to use:**
- **Fine-tuning:** Need specific tone, format, or domain expertise
- **RAG:** Need current information, cited sources, enterprise data
- **Agents:** Need to take actions, use tools, multi-step reasoning

**ðŸ”§ Real Project Application:**
> For our **SharePoint Document Chatbot**, we evaluated all three approaches:
> 
> | Approach | Evaluation | Result |
> |----------|------------|--------|
> | **Fine-tuning** | Trained on 10K company docs | âŒ Expensive, outdated in weeks as docs changed |
> | **RAG** | Real-time retrieval from Azure AI Search | âœ… Chose this - always current, cites sources |
> | **Agents** | LLM + SharePoint API tools | âš ï¸ Used for admin tasks only (too slow for chat) |
> 
> **Decision:** RAG for user-facing chat (fast, accurate, auditable), Agents for admin operations (document upload, permission sync).

---

## Q18: What are Hallucinations in LLMs? How to mitigate?

**Answer:**

**Hallucinations:** LLMs generating plausible but incorrect/fabricated information.

**Types:**
1. **Factual errors:** Wrong facts confidently stated
2. **Fabrication:** Made-up references, quotes, data
3. **Inconsistency:** Contradicting previous statements

**Mitigation Strategies:**

| Strategy | Description |
|----------|-------------|
| **RAG** | Ground responses in retrieved documents |
| **Temperature** | Lower temperature (0.0-0.3) for factual tasks |
| **Explicit instructions** | "Only answer based on provided context" |
| **Self-consistency** | Generate multiple answers, check agreement |
| **Fact verification** | Post-process with fact-checking |
| **Citations** | Force model to cite sources |

```python
system_prompt = """
You are a helpful assistant. IMPORTANT RULES:
1. Only answer based on the provided context
2. If the answer is not in the context, say "I don't have information about that"
3. Always cite the source document
4. Never make up information
"""
```

**ðŸ”§ Real Project Application:**
> We implemented a **multi-layer hallucination prevention system**:
> ```python
> # 1. Retrieval grounding - only use retrieved context
> context = await search_service.retrieve(query, top_k=5)
> 
> # 2. Low temperature for factual responses
> response = await openai_client.chat.completions.create(
>     model="gpt-4",
>     temperature=0.1,  # Low for factual accuracy
>     messages=[{"role": "system", "content": GROUNDED_SYSTEM_PROMPT}]
> )
> 
> # 3. Post-processing: Extract and verify citations
> citations = extract_citations(response.content)
> verified = verify_citations_exist_in_context(citations, context)
> 
> # 4. Confidence scoring
> if response contains "I'm not sure" or "based on my knowledge":
>     flag_for_review(response)
> ```
> This reduced user-reported hallucinations from 15% to under 2%.

---

# 5. RAG (Retrieval Augmented Generation)

## Q19: Explain RAG Architecture and Components

**Answer:**

**RAG combines retrieval with generation for knowledge-grounded responses.**

**Components:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAG Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Documentâ”‚â”€â”€â”€>â”‚  Chunking    â”‚â”€â”€â”€>â”‚  Embedding    â”‚  â”‚
â”‚  â”‚  Store  â”‚    â”‚  Strategy    â”‚    â”‚  Model        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                             â”‚          â”‚
â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                    â”‚  Vector Store   â”‚ â”‚
â”‚                                    â”‚  (Index)        â”‚ â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Query  â”‚â”€â”€â”€>â”‚Query Embeddingâ”‚â”€â”€â”€>â”‚  Retrieval   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  (Top-K)     â”‚  â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚          â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                 â”‚        LLM Generation              â”‚ â”‚
â”‚                 â”‚   (Query + Retrieved Context)      â”‚ â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Decisions:**
1. **Chunking:** Size (512-1024 tokens), overlap (10-20%)
2. **Embedding:** OpenAI ada-002, Cohere, sentence-transformers
3. **Vector Store:** Pinecone, Weaviate, Azure AI Search, FAISS
4. **Retrieval:** Semantic search, hybrid (keyword + vector)
5. **Generation:** GPT-4, Claude, Llama

**ðŸ”§ Real Project Application:**
> Our production RAG pipeline configuration:
> ```python
> RAG_CONFIG = {
>     "chunking": {
>         "strategy": "recursive",
>         "chunk_size": 800,  # tokens
>         "chunk_overlap": 100,
>         "separators": ["\n\n", "\n", ". ", " "]
>     },
>     "embedding": {
>         "model": "text-embedding-3-small",
>         "dimensions": 1536,
>         "batch_size": 100
>     },
>     "retrieval": {
>         "vector_store": "Azure AI Search",
>         "search_type": "hybrid",  # keyword + vector
>         "top_k": 5,
>         "semantic_reranking": True
>     },
>     "generation": {
>         "model": "gpt-4o",
>         "temperature": 0.1,
>         "max_tokens": 2000
>     }
> }
> ```
> This configuration processes 50K+ documents with 94% user satisfaction on answer quality.

---

## Q20: What are different Chunking Strategies?

**Answer:**

| Strategy | Description | Use Case |
|----------|-------------|----------|
| **Fixed-size** | Split by character/token count | Simple, consistent |
| **Sentence** | Split by sentence boundaries | Preserves meaning |
| **Paragraph** | Split by paragraphs | Structured docs |
| **Semantic** | Split by topic/meaning change | Best quality, complex |
| **Recursive** | Hierarchical splitting | Long documents |

**Best Practices:**
- Chunk size: 512-1024 tokens typical
- Overlap: 10-20% to preserve context
- Metadata: Keep source, page, section info

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ".", " "]
)
chunks = splitter.split_text(document)
```

**ðŸ”§ Real Project Application:**
> We implemented **content-aware chunking** based on document type:
> ```python
> def get_chunking_strategy(doc_type: str, content: str):
>     if doc_type == "pdf":
>         # PDFs: Respect page boundaries
>         return PageAwareChunker(chunk_size=800, overlap=100)
>     elif doc_type == "xlsx":
>         # Excel: Each sheet as separate context
>         return SheetBasedChunker()
>     elif doc_type == "code":
>         # Code: Respect function/class boundaries
>         return CodeAwareChunker(language="python")
>     else:
>         # Default: Recursive with semantic boundaries
>         return RecursiveCharacterTextSplitter(
>             chunk_size=800, chunk_overlap=100,
>             separators=["\n\n", "\n", ". "]
>         )
> ```
> This improved retrieval precision by 35% compared to one-size-fits-all chunking.

---

## Q21: How do you evaluate RAG systems?

**Answer:**

**Metrics:**

| Metric | Measures | How |
|--------|----------|-----|
| **Retrieval Precision** | Relevance of retrieved docs | % relevant in top-K |
| **Retrieval Recall** | Coverage of relevant docs | % of relevant docs retrieved |
| **MRR** | Ranking quality | 1/rank of first relevant doc |
| **NDCG** | Graded relevance ranking | Normalized discounted cumulative gain |
| **Answer Faithfulness** | Grounded in context | LLM judge / human eval |
| **Answer Relevance** | Answers the question | LLM judge / human eval |

**Evaluation Frameworks:**
- **RAGAS:** Automated RAG evaluation
- **LangSmith:** Tracing and evaluation
- **Human evaluation:** Gold standard

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision

result = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_precision]
)
```

**ðŸ”§ Real Project Application:**
> Our RAG evaluation pipeline runs weekly on 500 golden Q&A pairs:
> ```python
> # Automated evaluation metrics
> evaluation_results = {
>     "retrieval_precision@5": 0.82,  # 4.1/5 chunks relevant
>     "retrieval_recall": 0.91,        # Found 91% of relevant info
>     "answer_faithfulness": 0.94,     # 94% answers grounded in context
>     "answer_relevancy": 0.88,        # 88% answers address the question
>     "latency_p95": 2.3               # seconds
> }
> 
> # Human evaluation (monthly, 100 samples)
> human_eval = {
>     "accuracy": 0.92,
>     "helpfulness": 4.3/5,
>     "citation_accuracy": 0.89
> }
> ```
> When retrieval_precision dropped below 0.75 threshold, we automatically trigger alerts for investigation.

---

## Q22: What is Hybrid Search? When to use it?

**Answer:**

**Hybrid Search combines keyword (BM25) and semantic (vector) search.**

```python
# Hybrid scoring
final_score = Î± Ã— vector_score + (1-Î±) Ã— keyword_score
```

**When to use:**

| Use Case | Best Approach |
|----------|---------------|
| Exact matches needed (IDs, names) | Keyword |
| Semantic understanding | Vector |
| General enterprise search | Hybrid |
| Code search | Hybrid |

**Implementation (Azure AI Search):**
```python
results = search_client.search(
    search_text=query,  # Keyword search
    vector_queries=[VectorizedQuery(
        vector=query_embedding,
        k_nearest_neighbors=10,
        fields="content_vector"
    )],
    query_type="semantic",  # Add semantic ranking
    semantic_configuration_name="my-semantic-config"
)
```

**ðŸ”§ Real Project Application:**
> Hybrid search was crucial for handling **different query types**:
> ```python
> # Query: "policy document 2024" - needs keyword for "2024"
> # Query: "how do I request time off" - needs semantic understanding
> 
> # Our implementation
> async def hybrid_search(query: str, user_id: str):
>     # Generate embedding for semantic search
>     embedding = await get_embedding(query)
>     
>     # Build permission filter
>     security_filter = build_user_filter(user_id)
>     
>     results = search_client.search(
>         search_text=query,  # BM25 keyword search
>         vector_queries=[VectorizedQuery(
>             vector=embedding,
>             k_nearest_neighbors=50,
>             fields="content_vector"
>         )],
>         filter=security_filter,  # Permission enforcement
>         query_type="semantic",
>         semantic_configuration_name="default",
>         top=10
>     )
>     return results
> ```
> Hybrid search improved retrieval accuracy by 28% over pure vector search, especially for queries with specific terms.

---

# 6. Vector Databases & Embeddings

## Q23: Compare Vector Databases (Pinecone, Weaviate, Azure AI Search, FAISS)

**Answer:**

| Database | Type | Strengths | Best For |
|----------|------|-----------|----------|
| **Pinecone** | Managed cloud | Easy setup, scalable | Production, quick start |
| **Weaviate** | Open source | Hybrid search, GraphQL | Self-hosted, flexibility |
| **Azure AI Search** | Azure native | Enterprise, hybrid, security | Azure ecosystem |
| **FAISS** | Library | Fast, local, free | Prototyping, small scale |
| **Chroma** | Open source | Simple API, local | Development, small projects |
| **Qdrant** | Open source | Filtering, payloads | Complex filtering needs |

**Selection Criteria:**
- Scale: Cloud for large scale
- Cost: Open source for budget
- Features: Hybrid search, filtering, metadata
- Integration: Match your cloud provider

**ðŸ”§ Real Project Application:**
> Our vector database selection process:
> 
> | Criteria | Weight | Azure AI Search | Pinecone | FAISS |
> |----------|--------|-----------------|----------|-------|
> | Azure Integration | 30% | â­â­â­ | â­â­ | â­ |
> | Hybrid Search | 25% | â­â­â­ | â­â­ | âŒ |
> | Security Filtering | 25% | â­â­â­ | â­â­ | â­ |
> | Cost at 50K docs | 20% | $$ | $$$ | Free |
> 
> **Winner: Azure AI Search** because:
> 1. Native integration with Azure AD for permission filtering
> 2. Hybrid search (BM25 + vector) built-in
> 3. Semantic ranking as add-on layer
> 4. Already using Azure ecosystem (cost consolidation)

---

## Q24: Explain Similarity Metrics (Cosine, Euclidean, Dot Product)

**Answer:**

| Metric | Formula | Range | Use Case |
|--------|---------|-------|----------|
| **Cosine** | AÂ·B / (\|\|A\|\| Ã— \|\|B\|\|) | [-1, 1] | Normalized vectors, text |
| **Euclidean** | âˆš(Î£(A-B)Â²) | [0, âˆž) | Spatial data |
| **Dot Product** | Î£(AÃ—B) | (-âˆž, âˆž) | When magnitude matters |

**Best Practices:**
- **Cosine:** Most common for embeddings, direction matters
- **Euclidean:** When absolute distance matters
- **Dot Product:** OpenAI embeddings (already normalized)

```python
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def euclidean_distance(a, b):
    return np.linalg.norm(a - b)

def dot_product(a, b):
    return np.dot(a, b)
```

**ðŸ”§ Real Project Application:**
> We use **Cosine Similarity** in Azure AI Search because:
> ```python
> # OpenAI embeddings are normalized, so cosine = dot product
> # But cosine is more intuitive for debugging
> 
> # Example: Debugging why "vacation policy" didn't match "PTO guidelines"
> query_emb = get_embedding("vacation policy")
> doc_emb = get_embedding("PTO guidelines")
> 
> similarity = cosine_similarity(query_emb, doc_emb)
> # Result: 0.72 - decent but not top match
> 
> # Root cause: Missing synonym expansion
> # Solution: Added "vacation" â†’ "PTO", "time off" in query expansion
> expanded_query = expand_synonyms("vacation policy")  # "vacation policy PTO time off"
> # New similarity: 0.89 - now correctly ranked #1
> ```

---

## Q25: How do you handle embedding model updates in production?

**Answer:**

**Challenge:** New embedding model = all vectors incompatible

**Strategies:**

1. **Blue-Green Deployment:**
   - Create new index with new embeddings
   - Switch traffic when ready
   - Keep old index as fallback

2. **Gradual Migration:**
   - Re-embed documents in batches
   - Track migration progress
   - Use versioned index names

3. **Dual Index:**
   - Query both indexes during transition
   - Merge results
   - Phase out old index

```python
# Versioned index approach
INDEX_V1 = "documents-ada-002-v1"
INDEX_V2 = "documents-text-embedding-3-v2"

async def migrate_embeddings():
    documents = get_all_documents()
    for batch in chunks(documents, 100):
        new_embeddings = embedding_model_v2.embed(batch)
        index_v2.upsert(batch, new_embeddings)
        
# Feature flag for switching
if config.USE_NEW_EMBEDDINGS:
    results = search_v2(query)
else:
    results = search_v1(query)
```

**ðŸ”§ Real Project Application:**
> We migrated from `ada-002` to `text-embedding-3-small` using **blue-green deployment**:
> ```python
> # Phase 1: Create new index alongside old (Week 1)
> INDEX_OLD = "documents-ada-002"
> INDEX_NEW = "documents-embedding-3-small"
> 
> # Phase 2: Background re-indexing (Week 1-2)
> async def migrate_batch(doc_ids: List[str]):
>     for doc_id in doc_ids:
>         doc = await get_document(doc_id)
>         new_embedding = await new_model.embed(doc.content)
>         await index_new.upsert(doc_id, new_embedding, doc.metadata)
> 
> # Phase 3: A/B testing (Week 3)
> # 10% traffic to new index, compare metrics
> 
> # Phase 4: Full switch (Week 4)
> # Results: 15% better retrieval precision, 20% lower cost
> ```

---

# 7. Azure AI Services

## Q26: Explain Azure OpenAI Service architecture

**Answer:**

**Components:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Azure OpenAI Service                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Resource   â”‚    â”‚ Deployments â”‚   â”‚   Models    â”‚  â”‚
â”‚  â”‚  (Endpoint) â”‚â”€â”€â”€>â”‚   (Named)   â”‚â”€â”€>â”‚  (GPT-4,    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   etc.)     â”‚  â”‚
â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   Features                       â”‚   â”‚
â”‚  â”‚  â€¢ Content filtering  â€¢ Rate limiting            â”‚   â”‚
â”‚  â”‚  â€¢ Managed identity   â€¢ Private endpoints        â”‚   â”‚
â”‚  â”‚  â€¢ PTU (Provisioned)  â€¢ Regional availability    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Usage:**
```python
from openai import AzureOpenAI

client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2024-02-15-preview"
)

response = client.chat.completions.create(
    model="gpt-4-deployment",  # Deployment name, not model name
    messages=[{"role": "user", "content": "Hello!"}],
    temperature=0.7
)
```

**ðŸ”§ Real Project Application:**
> Our Azure OpenAI service configuration for production:
> ```python
> # config.py
> class AzureOpenAIConfig:
>     ENDPOINT = "https://mycompany-openai.openai.azure.com/"
>     API_VERSION = "2024-02-15-preview"
>     
>     # Multiple deployments for different purposes
>     DEPLOYMENTS = {
>         "chat": "gpt-4o",           # User-facing responses
>         "embedding": "text-embedding-3-small",  # Document indexing
>         "summarization": "gpt-4o-mini"  # Batch document summaries (cheaper)
>     }
>     
>     # Rate limiting per deployment
>     RATE_LIMITS = {
>         "gpt-4o": {"rpm": 60, "tpm": 80000},
>         "gpt-4o-mini": {"rpm": 200, "tpm": 200000}
>     }
> ```
> This multi-deployment strategy reduced costs by 40% while maintaining quality for user-facing interactions.

---

## Q27: Explain Azure AI Search for RAG

**Answer:**

**Key Features for RAG:**

1. **Vector Search:** Native vector indexing
2. **Hybrid Search:** Combine keyword + vector
3. **Semantic Ranking:** Re-rank with deep learning
4. **Security:** Row-level security, AAD integration

**Index Schema for RAG:**
```python
from azure.search.documents.indexes.models import (
    SearchIndex, SearchField, VectorSearch, 
    HnswAlgorithmConfiguration, SearchFieldDataType
)

index = SearchIndex(
    name="documents",
    fields=[
        SearchField(name="id", type=SearchFieldDataType.String, key=True),
        SearchField(name="content", type=SearchFieldDataType.String, searchable=True),
        SearchField(name="content_vector", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                   vector_search_dimensions=1536, vector_search_profile_name="myHnswProfile"),
        SearchField(name="owner_id", type=SearchFieldDataType.String, filterable=True),
        SearchField(name="allowed_users", type=SearchFieldDataType.Collection(SearchFieldDataType.String), filterable=True),
    ],
    vector_search=VectorSearch(
        algorithms=[HnswAlgorithmConfiguration(name="myHnsw")],
        profiles=[VectorSearchProfile(name="myHnswProfile", algorithm_configuration_name="myHnsw")]
    )
)
```

**ðŸ”§ Real Project Application:**
> Our production Azure AI Search index schema includes permission fields:
> ```python
> fields = [
>     # Core content fields
>     SearchField(name="id", type="Edm.String", key=True),
>     SearchField(name="content", type="Edm.String", searchable=True, analyzer="en.microsoft"),
>     SearchField(name="content_vector", type="Collection(Edm.Single)", 
>                 vector_search_dimensions=1536, vector_search_profile_name="hnsw"),
>     
>     # Metadata for filtering
>     SearchField(name="source_file", type="Edm.String", filterable=True),
>     SearchField(name="file_type", type="Edm.String", filterable=True, facetable=True),
>     SearchField(name="created_date", type="Edm.DateTimeOffset", filterable=True, sortable=True),
>     
>     # CRITICAL: Permission fields for security trimming
>     SearchField(name="owner_id", type="Edm.String", filterable=True),
>     SearchField(name="allowed_users", type="Collection(Edm.String)", filterable=True),
>     SearchField(name="allowed_groups", type="Collection(Edm.String)", filterable=True),
> ]
> ```
> This schema enables permission-aware search where users only see documents they're authorized to access.

---

## Q28: How do you implement security trimming in Azure AI Search?

**Answer:**

**Security Trimming:** Filtering search results based on user permissions.

**Implementation:**

1. **Store permissions in index:**
```python
{
    "id": "doc123",
    "content": "...",
    "owner_id": "user-abc",
    "allowed_users": ["user-abc", "user-xyz"],
    "allowed_groups": ["group-123", "group-456"]
}
```

2. **Build filter at query time:**
```python
def build_security_filter(user_id: str, group_ids: List[str]) -> str:
    filter_parts = [f"owner_id eq '{user_id}'"]
    filter_parts.append(f"allowed_users/any(u: u eq '{user_id}')")
    
    for group_id in group_ids:
        filter_parts.append(f"allowed_groups/any(g: g eq '{group_id}')")
    
    return " or ".join(filter_parts)

# Apply filter
results = search_client.search(
    search_text=query,
    filter=build_security_filter(user_id, user_groups),
    vector_queries=[vector_query]
)
```

**ðŸ”§ Real Project Application:**
> Our **production security trimming implementation** syncs with SharePoint permissions:
> ```python
> # 1. During ingestion - fetch SharePoint permissions via Graph API
> async def ingest_document(file_item: dict):
>     permissions = await graph_client.get_file_permissions(file_item["id"])
>     
>     document = {
>         "id": file_item["id"],
>         "content": extract_text(file_item),
>         "content_vector": await get_embedding(content),
>         "owner_id": permissions.get("owner_id"),
>         "allowed_users": [p["user_id"] for p in permissions.get("users", [])],
>         "allowed_groups": [p["group_id"] for p in permissions.get("groups", [])]
>     }
>     await search_client.upload_documents([document])
> 
> # 2. During search - build OData filter from user's Azure AD claims
> def build_security_filter(user: dict) -> str:
>     user_id = user["oid"]  # Azure AD Object ID
>     groups = user.get("groups", [])
>     
>     filters = [f"owner_id eq '{user_id}'"]
>     filters.append(f"allowed_users/any(u: u eq '{user_id}')")
>     for group_id in groups:
>         filters.append(f"allowed_groups/any(g: g eq '{group_id}')")
>     
>     return " or ".join(filters)
> 
> # 3. Logic App syncs permission changes every 5 minutes
> ```
> This ensures users only see documents they have SharePoint access to, maintaining security compliance.

---

# 8. MLOps & Production AI

## Q29: Explain ML Pipeline components

**Answer:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ML Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Data Pipeline:                                         â”‚
â”‚  [Ingestion] â†’ [Validation] â†’ [Preprocessing] â†’        â”‚
â”‚  [Feature Engineering] â†’ [Feature Store]               â”‚
â”‚                                                         â”‚
â”‚  Training Pipeline:                                     â”‚
â”‚  [Data Split] â†’ [Training] â†’ [Evaluation] â†’            â”‚
â”‚  [Model Registry] â†’ [Approval]                         â”‚
â”‚                                                         â”‚
â”‚  Deployment Pipeline:                                   â”‚
â”‚  [Model Loading] â†’ [Containerization] â†’ [Deployment] â†’ â”‚
â”‚  [A/B Testing] â†’ [Monitoring]                          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tools:**
- **Orchestration:** Airflow, Kubeflow, Azure ML Pipelines
- **Experiment Tracking:** MLflow, Weights & Biases, Neptune
- **Model Registry:** MLflow, Azure ML, SageMaker
- **Feature Store:** Feast, Tecton, Azure ML Feature Store

**ðŸ”§ Real Project Application:**
> Our RAG system uses a simplified but production-ready pipeline:
> ```
> SharePoint â†’ Logic App (Trigger) â†’ Ingestion Service â†’ Azure AI Search
>      â†“                                    â†“
> Permission Changes              Document Processing
>      â†“                                    â†“
> Delta Sync API â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Embedding Generation
> ```
> 
> **Key Components:**
> 1. **Ingestion Trigger:** Logic App monitors SharePoint for changes
> 2. **Document Processing:** FastAPI service extracts text (PDF, DOCX, XLSX)
> 3. **Embedding Pipeline:** Batch embedding with rate limiting
> 4. **Index Management:** Upsert to Azure AI Search with versioning
> 5. **Monitoring:** Application Insights tracks ingestion success/failure rates

---

## Q30: How do you monitor LLM applications in production?

**Answer:**

**Key Metrics:**

| Category | Metrics |
|----------|---------|
| **Latency** | P50, P95, P99 response time |
| **Throughput** | Requests/second, tokens/second |
| **Errors** | Error rate, timeout rate |
| **Cost** | Tokens used, cost per query |
| **Quality** | User feedback, hallucination rate |

**Monitoring Stack:**
```python
# Tracing with LangSmith/Langfuse
from langsmith import traceable

@traceable(name="chat_query")
async def process_query(query: str, user_id: str):
    # Track retrieval
    with trace_span("retrieval"):
        chunks = retrieve(query)
    
    # Track generation
    with trace_span("generation"):
        response = generate(query, chunks)
    
    # Log metrics
    log_metric("tokens_used", response.usage.total_tokens)
    log_metric("retrieval_count", len(chunks))
    
    return response
```

**Alerting:**
- High latency (>5s P95)
- Error rate spike
- Token usage anomaly
- Low user satisfaction

**ðŸ”§ Real Project Application:**
> Our production monitoring dashboard tracks:
> ```python
> # Custom metrics logged to Application Insights
> @traceable(name="rag_query")
> async def process_query(query: str, user_id: str):
>     start_time = time.time()
>     
>     # Track retrieval metrics
>     chunks = await retrieve(query, user_id)
>     track_metric("retrieval_count", len(chunks))
>     track_metric("retrieval_latency_ms", (time.time() - start_time) * 1000)
>     
>     # Track generation metrics
>     gen_start = time.time()
>     response = await generate(query, chunks)
>     track_metric("generation_latency_ms", (time.time() - gen_start) * 1000)
>     track_metric("tokens_used", response.usage.total_tokens)
>     track_metric("estimated_cost_usd", calculate_cost(response.usage))
>     
>     # Total latency
>     track_metric("total_latency_ms", (time.time() - start_time) * 1000)
>     
>     return response
> 
> # Alerts configured:
> # - P95 latency > 3000ms â†’ PagerDuty alert
> # - Error rate > 5% in 5 minutes â†’ Slack notification
> # - Daily cost > $100 â†’ Email to team lead
> ```

---

## Q31: Explain A/B testing for ML models

**Answer:**

**Setup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Load Balancer                 â”‚
â”‚         (Traffic Splitting)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚  Model A  â”‚    â”‚  Model B  â”‚
        â”‚  (90%)    â”‚    â”‚  (10%)    â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚                â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Metrics Store  â”‚
              â”‚  (Compare A/B)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Considerations:**
1. **Statistical significance:** Minimum sample size
2. **Metrics:** Define primary/secondary metrics
3. **Duration:** Run long enough for confidence
4. **Segmentation:** User groups, regions

```python
# Feature flag based routing
def get_model_version(user_id: str) -> str:
    if is_in_experiment(user_id, "new_model_test"):
        return "model_v2" if hash(user_id) % 100 < 10 else "model_v1"
    return "model_v1"

# Track metrics by variant
async def process_with_tracking(query, user_id):
    variant = get_model_version(user_id)
    response = await model_router[variant].generate(query)
    
    log_experiment_metric(
        experiment="new_model_test",
        variant=variant,
        user_id=user_id,
        metrics={"latency": response.latency, "satisfaction": ...}
    )
    return response
```

**ðŸ”§ Real Project Application:**
> We A/B tested **GPT-4 vs GPT-4o** for answer generation:
> ```python
> # Experiment setup
> EXPERIMENTS = {
>     "gpt4o_migration": {
>         "control": "gpt-4",
>         "treatment": "gpt-4o",
>         "traffic_split": 0.2,  # 20% to GPT-4o
>         "metrics": ["latency", "user_thumbs_up", "token_cost"]
>     }
> }
> 
> # Results after 2 weeks (10K queries):
> # GPT-4:  Latency 2.1s, Satisfaction 87%, Cost $0.04/query
> # GPT-4o: Latency 1.4s, Satisfaction 89%, Cost $0.025/query
> # Decision: Migrate to GPT-4o (33% faster, 2% better satisfaction, 40% cheaper)
> ```

---

# 9. Python & Frameworks

## Q32: Explain async/await in Python for AI applications

**Answer:**

**Why async matters for AI:**
- I/O bound operations (API calls, database)
- Handle multiple concurrent requests
- Don't block while waiting for LLM responses

```python
import asyncio
import httpx

# Synchronous - slow
def sync_embeddings(texts):
    results = []
    for text in texts:
        response = requests.post(EMBEDDING_API, json={"input": text})
        results.append(response.json())
    return results  # Takes N * latency

# Asynchronous - fast
async def async_embeddings(texts):
    async with httpx.AsyncClient() as client:
        tasks = [
            client.post(EMBEDDING_API, json={"input": text})
            for text in texts
        ]
        responses = await asyncio.gather(*tasks)
        return [r.json() for r in responses]  # Takes ~1 * latency

# Usage in FastAPI
@app.post("/embed")
async def embed_documents(docs: List[str]):
    embeddings = await async_embeddings(docs)
    return {"embeddings": embeddings}
```

**ðŸ”§ Real Project Application:**
> Async was critical for our **document ingestion service**:
> ```python
> # BEFORE: Sequential processing - 100 docs took 5 minutes
> for doc in documents:
>     text = extract_text(doc)  # 1s
>     embedding = get_embedding(text)  # 0.5s
>     await index.upsert(doc, embedding)  # 0.5s
> # Total: 100 * 2s = 200s
> 
> # AFTER: Async concurrent processing - 100 docs in 30 seconds
> async def process_document(doc):
>     text = await extract_text_async(doc)
>     embedding = await get_embedding_async(text)
>     return (doc.id, embedding)
> 
> # Process in batches of 10 concurrent
> results = await asyncio.gather(*[
>     process_document(doc) for doc in documents
> ])
> await index.upsert_batch(results)
> # Total: ~30s (10x faster!)
> ```
> This async architecture handles our daily ingestion of 500+ document updates without blocking the API.

---

## Q33: Explain FastAPI best practices for AI services

**Answer:**

```python
from fastapi import FastAPI, Depends, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import asyncio

app = FastAPI(title="AI Service", version="1.0.0")

# 1. CORS for frontend
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"])

# 2. Pydantic models for validation
class QueryRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    
class QueryResponse(BaseModel):
    answer: str
    sources: List[dict]
    latency_ms: float

# 3. Dependency injection
async def get_current_user(token: str = Depends(oauth2_scheme)):
    return await verify_token(token)

# 4. Async endpoints
@app.post("/query", response_model=QueryResponse)
async def query(
    request: QueryRequest,
    user: dict = Depends(get_current_user),
    background_tasks: BackgroundTasks
):
    start = time.time()
    
    # Async operations
    chunks = await retriever.search(request.query, request.top_k)
    answer = await llm.generate(request.query, chunks)
    
    # Background logging
    background_tasks.add_task(log_query, user["id"], request.query)
    
    return QueryResponse(
        answer=answer,
        sources=chunks,
        latency_ms=(time.time() - start) * 1000
    )

# 5. Streaming responses
@app.post("/stream")
async def stream_response(request: QueryRequest):
    async def generate():
        async for chunk in llm.stream(request.query):
            yield f"data: {chunk}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")

# 6. Health check
@app.get("/health")
async def health():
    return {"status": "healthy", "model_loaded": model is not None}
```

**ðŸ”§ Real Project Application:**
> Our FastAPI service structure:
> ```
> backend/
> â”œâ”€â”€ app/
> â”‚   â”œâ”€â”€ main.py              # FastAPI app, CORS, routers
> â”‚   â”œâ”€â”€ config.py            # Environment configuration
> â”‚   â”œâ”€â”€ api/
> â”‚   â”‚   â””â”€â”€ routes/
> â”‚   â”‚       â”œâ”€â”€ chat.py      # /api/chat/query endpoint
> â”‚   â”‚       â”œâ”€â”€ documents.py # /api/documents/* endpoints
> â”‚   â”‚       â””â”€â”€ ingestion.py # /api/ingestion/* endpoints
> â”‚   â”œâ”€â”€ services/
> â”‚   â”‚   â”œâ”€â”€ azure_search_service.py
> â”‚   â”‚   â”œâ”€â”€ openai_service.py
> â”‚   â”‚   â””â”€â”€ auth_service.py
> â”‚   â””â”€â”€ middleware/
> â”‚       â”œâ”€â”€ auth_middleware.py   # JWT validation
> â”‚       â””â”€â”€ logging_middleware.py # Request/response logging
> ```
> 
> Key patterns used:
> - **Dependency Injection:** `Depends(get_current_user)` for auth
> - **Background Tasks:** Async logging without blocking response
> - **Streaming:** Server-Sent Events for real-time chat
> - **Health Checks:** `/health` endpoint for Kubernetes probes

---

## Q34: How do you handle errors and retries in LLM applications?

**Answer:**

```python
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import httpx

# Retry decorator
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type((httpx.TimeoutException, httpx.HTTPStatusError))
)
async def call_llm_with_retry(prompt: str):
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.post(
            LLM_ENDPOINT,
            json={"prompt": prompt},
            headers={"Authorization": f"Bearer {API_KEY}"}
        )
        response.raise_for_status()
        return response.json()

# Circuit breaker pattern
class CircuitBreaker:
    def __init__(self, failure_threshold=5, reset_timeout=60):
        self.failures = 0
        self.threshold = failure_threshold
        self.reset_timeout = reset_timeout
        self.last_failure = None
        self.state = "closed"
    
    async def call(self, func, *args, **kwargs):
        if self.state == "open":
            if time.time() - self.last_failure > self.reset_timeout:
                self.state = "half-open"
            else:
                raise Exception("Circuit breaker is open")
        
        try:
            result = await func(*args, **kwargs)
            if self.state == "half-open":
                self.state = "closed"
                self.failures = 0
            return result
        except Exception as e:
            self.failures += 1
            self.last_failure = time.time()
            if self.failures >= self.threshold:
                self.state = "open"
            raise

# Usage
breaker = CircuitBreaker()
result = await breaker.call(call_llm_with_retry, "Hello")
```

**ðŸ”§ Real Project Application:**
> Our production error handling for Azure OpenAI:
> ```python
> from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception
> 
> # Retry configuration for Azure OpenAI calls
> @retry(
>     stop=stop_after_attempt(3),
>     wait=wait_exponential(multiplier=1, min=2, max=30),
>     retry=retry_if_exception(lambda e: 
>         isinstance(e, RateLimitError) or 
>         isinstance(e, APIConnectionError) or
>         (isinstance(e, APIStatusError) and e.status_code >= 500)
>     ),
>     before_sleep=lambda retry_state: logger.warning(
>         f"Retry {retry_state.attempt_number}: {retry_state.outcome.exception()}"
>     )
> )
> async def call_openai_with_retry(messages: list, **kwargs):
>     return await openai_client.chat.completions.create(
>         messages=messages,
>         timeout=30.0,
>         **kwargs
>     )
> 
> # Graceful degradation
> async def get_response(query: str):
>     try:
>         return await call_openai_with_retry(messages)
>     except Exception as e:
>         logger.error(f"OpenAI failed after retries: {e}")
>         return "I'm experiencing technical difficulties. Please try again shortly."
> ```
> This prevented 99% of transient failures from reaching users.

---

# 10. System Design for AI

## Q35: Design a scalable RAG chatbot system

**Answer:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Architecture                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Users     â”‚â”€â”€â”€â”€>â”‚   CDN       â”‚â”€â”€â”€â”€>â”‚   Frontend          â”‚   â”‚
â”‚  â”‚             â”‚     â”‚   (Static)  â”‚     â”‚   (React/Next.js)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                      â”‚              â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                              â”‚       API Gateway / Load Balancer â”‚  â”‚
â”‚                              â”‚       (Rate limiting, Auth)       â”‚  â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                      â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Backend Services (K8s)                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Chat API   â”‚  â”‚ Ingestion   â”‚  â”‚   Admin API         â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  (FastAPI)  â”‚  â”‚ Service     â”‚  â”‚                     â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Data Layer                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚Azure AI     â”‚  â”‚   Redis     â”‚  â”‚   PostgreSQL        â”‚   â”‚  â”‚
â”‚  â”‚  â”‚Search       â”‚  â”‚   (Cache)   â”‚  â”‚   (Metadata)        â”‚   â”‚  â”‚
â”‚  â”‚  â”‚(Vectors)    â”‚  â”‚             â”‚  â”‚                     â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    External Services                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚Azure OpenAI â”‚  â”‚ SharePoint  â”‚  â”‚   Azure AD          â”‚   â”‚  â”‚
â”‚  â”‚  â”‚(LLM, Embed) â”‚  â”‚ (Source)    â”‚  â”‚   (Auth)            â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Decisions:**

1. **Caching:**
   - Cache embeddings for repeated queries
   - Cache LLM responses for identical queries
   - Cache user permissions

2. **Scaling:**
   - Horizontal scaling of API servers
   - Separate ingestion workers
   - Auto-scaling based on load

3. **Reliability:**
   - Retry with exponential backoff
   - Circuit breakers for external services
   - Fallback responses

**ðŸ”§ Real Project Application:**
> Our production architecture for the **SharePoint RAG Chatbot**:
> ```
> â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
> â”‚                   Production Architecture                       â”‚
> â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
> â”‚                                                                 â”‚
> â”‚  Users â”€â”€â–º Azure AD â”€â”€â–º React Chat App â”€â”€â–º API Gateway         â”‚
> â”‚                              â”‚                  â”‚               â”‚
> â”‚                              â””â”€â”€â–º Admin App     â”‚               â”‚
> â”‚                                                 â–¼               â”‚
> â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
> â”‚  â”‚              FastAPI Backend (Azure App Service)        â”‚   â”‚
> â”‚  â”‚                                                         â”‚   â”‚
> â”‚  â”‚  /api/chat/query â—„â”€â”€â–º Azure OpenAI (GPT-4o)            â”‚   â”‚
> â”‚  â”‚         â”‚                                               â”‚   â”‚
> â”‚  â”‚         â–¼                                               â”‚   â”‚
> â”‚  â”‚  Azure AI Search â—„â”€â”€â–º Permission Filter                â”‚   â”‚
> â”‚  â”‚  (Hybrid + Vectors)     (user_id, groups)              â”‚   â”‚
> â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
> â”‚                    â”‚                                           â”‚
> â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
> â”‚  â”‚              Data Sync Layer                              â”‚ â”‚
> â”‚  â”‚                                                           â”‚ â”‚
> â”‚  â”‚  SharePoint â”€â”€â–º Logic App â”€â”€â–º Ingestion Service          â”‚ â”‚
> â”‚  â”‚  (Source)       (Trigger)     (Process + Index)          â”‚ â”‚
> â”‚  â”‚                                                           â”‚ â”‚
> â”‚  â”‚  Graph API â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Permission Sync             â”‚ â”‚
> â”‚  â”‚  (Delta Query)               (Every 5 min)               â”‚ â”‚
> â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
> â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
> ```
> This architecture handles 1000+ queries/day with 99.5% uptime.

---

## Q36: How do you handle rate limiting and cost control for LLM APIs?

**Answer:**

```python
import asyncio
from collections import defaultdict
import time

class RateLimiter:
    def __init__(self, requests_per_minute: int, tokens_per_minute: int):
        self.rpm = requests_per_minute
        self.tpm = tokens_per_minute
        self.requests = []
        self.tokens = []
        self.lock = asyncio.Lock()
    
    async def acquire(self, estimated_tokens: int = 1000):
        async with self.lock:
            now = time.time()
            minute_ago = now - 60
            
            # Clean old entries
            self.requests = [t for t in self.requests if t > minute_ago]
            self.tokens = [(t, n) for t, n in self.tokens if t > minute_ago]
            
            # Check limits
            current_requests = len(self.requests)
            current_tokens = sum(n for _, n in self.tokens)
            
            if current_requests >= self.rpm:
                wait_time = self.requests[0] - minute_ago
                await asyncio.sleep(wait_time)
            
            if current_tokens + estimated_tokens > self.tpm:
                # Wait or reject
                raise Exception("Token limit exceeded")
            
            self.requests.append(now)
            self.tokens.append((now, estimated_tokens))
    
    def record_actual_tokens(self, tokens: int):
        # Update with actual token usage
        pass

# Cost tracking
class CostTracker:
    PRICING = {
        "gpt-4": {"input": 0.03, "output": 0.06},  # per 1K tokens
        "gpt-3.5-turbo": {"input": 0.001, "output": 0.002},
        "text-embedding-3-small": {"input": 0.00002}
    }
    
    def __init__(self):
        self.usage = defaultdict(lambda: {"input": 0, "output": 0, "cost": 0})
    
    def track(self, model: str, input_tokens: int, output_tokens: int = 0):
        pricing = self.PRICING.get(model, {"input": 0, "output": 0})
        cost = (input_tokens / 1000 * pricing["input"] + 
                output_tokens / 1000 * pricing["output"])
        
        self.usage[model]["input"] += input_tokens
        self.usage[model]["output"] += output_tokens
        self.usage[model]["cost"] += cost
        
        return cost
    
    def get_daily_cost(self) -> float:
        return sum(u["cost"] for u in self.usage.values())

# Usage
limiter = RateLimiter(requests_per_minute=60, tokens_per_minute=90000)
cost_tracker = CostTracker()

async def call_llm(prompt: str):
    await limiter.acquire(estimated_tokens=2000)
    
    response = await openai_client.chat.completions.create(...)
    
    cost = cost_tracker.track(
        "gpt-4",
        response.usage.prompt_tokens,
        response.usage.completion_tokens
    )
    
    # Alert if daily cost exceeds threshold
    if cost_tracker.get_daily_cost() > DAILY_BUDGET:
        send_alert("Daily budget exceeded!")
    
    return response
```

**ðŸ”§ Real Project Application:**
> Our cost control implementation:
> ```python
> # Daily budget enforcement
> DAILY_BUDGET_USD = 50.0
> 
> class CostController:
>     def __init__(self):
>         self.daily_cost = 0.0
>         self.last_reset = datetime.now().date()
>     
>     def check_budget(self, estimated_tokens: int) -> bool:
>         # Reset daily counter
>         if datetime.now().date() > self.last_reset:
>             self.daily_cost = 0.0
>             self.last_reset = datetime.now().date()
>         
>         estimated_cost = (estimated_tokens / 1000) * 0.03  # GPT-4 pricing
>         if self.daily_cost + estimated_cost > DAILY_BUDGET_USD:
>             logger.warning(f"Daily budget exceeded: ${self.daily_cost:.2f}")
>             return False
>         return True
>     
>     def record_cost(self, usage: dict):
>         cost = (usage["prompt_tokens"] / 1000 * 0.03 + 
>                 usage["completion_tokens"] / 1000 * 0.06)
>         self.daily_cost += cost
> 
> # Tiered model selection based on query complexity
> def select_model(query: str, context_length: int) -> str:
>     if context_length > 4000 or "summarize" in query.lower():
>         return "gpt-4o"  # Full power for complex tasks
>     return "gpt-4o-mini"  # Cheaper for simple queries
> ```
> This reduced our monthly Azure OpenAI costs from $800 to $450.

---

# 11. Behavioral & Scenario Questions

## Q37: Tell me about a challenging ML project you worked on

**Answer Framework (STAR):**

**Situation:** "At [Company], we needed to build a document Q&A system that could handle 500K documents with strict security requirements."

**Task:** "I was responsible for designing the RAG architecture and implementing the permission-aware search system."

**Action:**
- Evaluated vector databases (chose Azure AI Search for enterprise features)
- Designed chunking strategy for various document types
- Implemented security trimming using Azure AD groups
- Built async ingestion pipeline for performance
- Created evaluation framework to measure accuracy

**Result:**
- Reduced search latency from 5s to 200ms
- Achieved 95% user satisfaction in pilot
- System handles 1000 queries/hour with 99.9% uptime

**ðŸ”§ My Real Project Story:**
> **Situation:** "At my company, we needed to build an Enterprise RAG Chatbot that lets employees search and query internal SharePoint documents while respecting document-level permissions - a user should only see results from documents they have access to."
> 
> **Task:** "I was the lead AI engineer responsible for the entire solution - from architecture design to production deployment."
> 
> **Action:**
> 1. **Architecture Design:** Chose Azure AI Search for hybrid search + security filtering, Azure OpenAI for embeddings and generation
> 2. **Permission Challenge:** Integrated Microsoft Graph API to fetch SharePoint permissions during ingestion, stored them as filterable fields
> 3. **Sync Problem:** Built Logic App workflow with Delta Query to sync permission changes every 5 minutes
> 4. **Performance:** Implemented async document processing, batch embeddings, and Redis caching for user permissions
> 5. **Quality:** Created evaluation pipeline with 500 golden Q&A pairs, automated weekly testing
> 
> **Result:**
> - Deployed to 200+ users across 3 departments
> - 94% user satisfaction score
> - Average query latency: 1.8 seconds (including auth + retrieval + generation)
> - Zero security incidents - permissions correctly enforced
> - 50K+ documents indexed from SharePoint

---

## Q38: How do you stay updated with AI/ML developments?

**Answer:**

**Resources:**
1. **Papers:** arXiv, Papers with Code, Semantic Scholar
2. **News:** The Batch (Andrew Ng), AI News
3. **Communities:** Twitter/X, Reddit r/MachineLearning, Discord
4. **Courses:** Coursera, fast.ai, Hugging Face courses
5. **Conferences:** NeurIPS, ICML, ACL, EMNLP proceedings
6. **Hands-on:** Kaggle competitions, personal projects

**Recent examples:**
- "I've been following the developments in mixture-of-experts models like Mixtral"
- "Recently implemented structured outputs from GPT-4 after reading the release notes"
- "Experimenting with new embedding models like text-embedding-3"

---

## Q39: How do you explain AI concepts to non-technical stakeholders?

**Answer:**

**Techniques:**

1. **Analogies:**
   - RAG: "Like giving the AI a reference book to look up answers instead of relying on memory"
   - Embeddings: "Converting words into coordinates on a map where similar meanings are close together"
   - Fine-tuning: "Like sending the AI to specialized training school"

2. **Business terms:**
   - Instead of "accuracy": "How often the system gets it right"
   - Instead of "latency": "Response time"
   - Instead of "tokens": "Words processed"

3. **Visuals:**
   - Flowcharts for pipelines
   - Before/after comparisons
   - ROI calculations

**Example:**
"Our RAG system is like a very smart research assistant. When you ask a question, it first searches through all our company documents to find relevant information, then uses that information to write a clear answer. This means it can only tell you things that are actually in our documents, reducing the risk of made-up answers."

**ðŸ”§ Real Project Application:**
> **Actual stakeholder presentation I gave:**
> 
> *"Think of our new chatbot as a super-powered search assistant for SharePoint.*
> 
> *When you ask 'What's our vacation policy?', here's what happens:*
> 1. *It searches through ALL our HR documents (like a librarian who knows every book)*
> 2. *It finds the 3-5 most relevant sections*
> 3. *It reads those sections and writes you a clear, natural answer*
> 4. *It shows you exactly which documents the answer came from - so you can verify*
> 
> *The key difference from ChatGPT: Our system can ONLY answer using YOUR company documents. It won't make things up because it can only quote what's actually written in SharePoint.*
> 
> *Security: Just like SharePoint, you only see documents you have access to. If you can't open a document in SharePoint, the chatbot won't include it in answers either."*
> 
> This explanation helped secure executive buy-in and a $50K budget increase for the project.

---

## Q40: What would you do if an LLM is giving incorrect answers in production?

**Answer:**

**Immediate Response:**
1. Check error logs and recent changes
2. Identify pattern (all queries? specific topics? certain users?)
3. Enable verbose logging for debugging
4. Consider temporary fallback (human review, cached responses)

**Investigation:**
```python
# Add debugging
async def debug_query(query: str):
    # 1. Check retrieval
    chunks = await retriever.search(query)
    logger.info(f"Retrieved {len(chunks)} chunks")
    for i, chunk in enumerate(chunks):
        logger.info(f"Chunk {i}: {chunk['content'][:100]}...")
        logger.info(f"Score: {chunk['score']}")
    
    # 2. Check prompt
    prompt = build_prompt(query, chunks)
    logger.info(f"Full prompt: {prompt}")
    
    # 3. Check LLM response
    response = await llm.generate(prompt, temperature=0)
    logger.info(f"Response: {response}")
    
    return response
```

**Root Cause Categories:**
1. **Retrieval issues:** Wrong chunks, poor embedding quality
2. **Prompt issues:** Unclear instructions, missing context
3. **Model issues:** Hallucination, outdated knowledge
4. **Data issues:** Stale content, permission problems

**Long-term Fixes:**
- Add evaluation suite
- Implement monitoring for quality metrics
- Create feedback loop for continuous improvement
- A/B test prompt changes

**ðŸ”§ Real Project Application:**
> **Actual debugging session I did:**
> 
> **Problem:** Users reported the chatbot was giving wrong answers about "employee benefits"
> 
> **Investigation:**
> ```python
> # Step 1: Reproduce and log
> query = "What are our health insurance options?"
> chunks = await search_service.retrieve(query, user_id="test_user")
> 
> # Found: Retrieved chunks were from 2022 benefits doc, not 2024!
> for chunk in chunks:
>     print(f"Score: {chunk.score}, Source: {chunk.source}, Date: {chunk.modified}")
> # Chunk 1: benefits_2022.pdf (score: 0.89)
> # Chunk 2: benefits_2024.pdf (score: 0.85)
> 
> # Step 2: Root cause - recency not factored in ranking
> # Step 3: Solution - add date boost to search
> ```
> 
> **Fix Applied:**
> ```python
> # Added freshness boosting to search query
> search_results = search_client.search(
>     search_text=query,
>     scoring_profile="freshness_boost",  # New scoring profile
>     filter=f"modified_date ge {one_year_ago}"  # Optional filter
> )
> ```
> 
> **Result:** Benefits queries now correctly return 2024 documents. Added monitoring to alert when documents older than 1 year are returned for policy-related queries.

---

# Quick Reference Card

## Common Interview Topics Checklist

- [ ] ML fundamentals (bias-variance, regularization, metrics)
- [ ] Deep learning (backprop, attention, transformers)
- [ ] NLP (embeddings, BERT, tokenization)
- [ ] LLMs (GPT, prompting, hallucinations)
- [ ] RAG (architecture, chunking, evaluation)
- [ ] Vector databases (comparison, similarity metrics)
- [ ] Azure AI services (OpenAI, AI Search, security)
- [ ] MLOps (pipelines, monitoring, A/B testing)
- [ ] Python (async, FastAPI, error handling)
- [ ] System design (scalability, cost, reliability)

## Key Numbers to Remember

| Metric | Typical Value |
|--------|---------------|
| BERT hidden size | 768 |
| GPT-4 context | 128K tokens |
| Embedding dimensions | 1536 (OpenAI ada-002), 3072 (text-embedding-3-large) |
| Chunk size | 512-1024 tokens |
| Chunk overlap | 10-20% |
| Top-K retrieval | 3-10 chunks |
| Temperature for facts | 0.0-0.3 |
| Temperature for creativity | 0.7-1.0 |

---

# 12. Bonus: Real Project Metrics & Learnings

## Key Metrics from My RAG Chatbot Project

| Metric | Before Optimization | After Optimization |
|--------|---------------------|-------------------|
| Query Latency (P95) | 4.2s | 1.8s |
| Retrieval Precision@5 | 0.65 | 0.82 |
| Answer Faithfulness | 0.78 | 0.94 |
| User Satisfaction | 72% | 94% |
| Monthly Azure Cost | $1,200 | $650 |
| Documents Indexed | 10K | 50K |
| Daily Active Users | 50 | 200+ |

## Top 5 Lessons Learned

1. **Permissions are HARD:** Syncing SharePoint permissions took 40% of development time. Use Delta Query API, not full sync.

2. **Chunking matters more than embedding model:** Spent 2 weeks optimizing embeddings, then 2 days on chunking strategy that gave bigger improvement.

3. **Users don't read citations:** Added automatic source highlighting and document preview - engagement with sources increased 3x.

4. **Monitor token costs daily:** Our first week cost $300/day before we added caching and tiered model selection.

5. **Test with real users early:** Our "perfect" prompt failed on real queries. User testing revealed edge cases we never imagined.

---

**Good luck with your interview! ðŸš€**

*Remember: Be confident, give concrete examples from YOUR projects, and don't be afraid to say "I don't know, but here's how I'd find out."*

*The best answers show you can **apply** concepts, not just **explain** them.*
